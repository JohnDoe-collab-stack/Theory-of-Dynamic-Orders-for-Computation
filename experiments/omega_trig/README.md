# Î©-Trig Experiment

> **Experimental validation of the Dynamic Order Theory framework**

This experiment tests whether a neural network T_Î¸ can learn both:
1. The **static truth** of a fixed structure Î©
2. The **dynamic order** of how truths emerge over time (encoded by kernel K)

---

## What is This About?

### The Core Question

In standard ML, we train models to predict labels. But can a model also learn **when** different facts become "knowable" - i.e., the temporal structure of knowledge acquisition?

### The Setup

- **Î© (Omega)**: A fixed "world" of trigonometric facts (e.g., "sin(45Â°) â‰¥ 0")
- **K (Kernel)**: A dynamic process that reveals facts over time (monotone refinement)
- **T_Î¸ (Theory)**: A neural network that learns to approximate both Î© and K

### The Key Finding

> **K's temporal structure is REAL and LEARNABLE.**
> 
> When we ask T_Î¸ to predict both the truth (y*) and the difficulty class (halt_rank), 
> it achieves 99% accuracy on both - but only when K's structure is preserved.
> Shuffling K's assignments destroys the halt prediction (â†’ 50% random chance).

---

## Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      Î© (Fixed World)                        â”‚
â”‚                                                             â”‚
â”‚  360 angles Ã— 8 question types = 2880 facts                 â”‚
â”‚  Example: "Is sin(45Â°) â‰¥ 0.5?" â†’ True                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      K (Dynamic Kernel)                     â”‚
â”‚                                                             â”‚
â”‚  Simulates a refinement process over time:                  â”‚
â”‚  - approx_t(x): interval approximation at time t            â”‚
â”‚  - val_t(x,i): truth value at time t (monotone: 0â†’1 only)   â”‚
â”‚  - t_first^K(Ïƒ): first time fact Ïƒ becomes true             â”‚
â”‚  - halt_rank: EARLY / MID / LATE / NEVER                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      T_Î¸ (Neural Theory)                    â”‚
â”‚                                                             â”‚
â”‚  Input: (angle, question_type)                              â”‚
â”‚  Output: y_hat (truth prediction) + halt_logits (4 classes) â”‚
â”‚  Loss: BCE(y*) + Î» Â· CE(halt_rank_K)                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Key Results

### Multi-Task Validation (3 seeds, Î»=0.5)

| Condition | Y Accuracy | Halt Accuracy | Î” Halt |
|-----------|------------|---------------|--------|
| **K-real** | 99.2% Â± 0.3% | **98.7% Â± 0.4%** | - |
| **Shuffle** | 98.9% Â± 0.5% | 48.1% Â± 2.6% | **-50.6pp** |

**Interpretation**: When K's structure is shuffled, halt prediction drops to random chance (~50%), while truth prediction remains high. This proves K encodes real information.

### Confusion Matrix (K-real)

| True \ Pred | EARLY | MID | LATE | NEVER |
|-------------|-------|-----|------|-------|
| **EARLY** | 99 | 2 | 0 | 0 |
| **MID** | 2 | 135 | 0 | 0 |
| **LATE** | 0 | 0 | 0 | 0 |
| **NEVER** | 0 | 0 | 0 | 195 |

**Overall: 99.1%** - The model correctly classifies all difficulty levels, not just the majority class.

### Î» Sweep

| Î»_halt | K Halt | Shuffle Halt | Î” |
|--------|--------|--------------|---|
| 0.0 | 4% | 13% | -9pp |
| 0.1 | 98% | 50% | **+48pp** |
| 0.5 | 99% | 50% | **+49pp** |
| 1.0 | 99% | 50% | **+49pp** |

**Interpretation**: Without the halt objective (Î»=0), no one learns. With Î»>0, K-real succeeds while Shuffle fails.

---

## File Structure

```
omega_trig/
â”œâ”€â”€ README.md                 # This file
â”œâ”€â”€ trig_kernel.py            # Î©-syntax: angles, profiles, questions
â”œâ”€â”€ dataset_trig.py           # Dataset generation and splitting
â”œâ”€â”€ model_T.py                # Basic T_Î¸ model
â”œâ”€â”€ dynamic_trig_kernel.py    # K: monotone refinement process
â”œâ”€â”€ pvec_trig.py              # P_vec: cut/bit/halt classification
â”‚
â”œâ”€â”€ train_T.py                # Baseline training (y* only)
â”œâ”€â”€ train_T_curriculum.py     # Curriculum training (weighted by K)
â”œâ”€â”€ train_T_multitask.py      # Multi-task: y* + halt_rank_K
â”‚
â”œâ”€â”€ analysis_T.py             # Theory gradient analysis
â”œâ”€â”€ analysis_pvec_trig.py     # P_vec linear probes
â”œâ”€â”€ sync_K_T.py               # K â†” T_Î¸ synchronization
â”‚
â”œâ”€â”€ run_ablation.py           # Curriculum ablation (multi-seed)
â”œâ”€â”€ run_mt_validation.py      # Multi-task validation + Î» sweep
â”œâ”€â”€ visualize_results.py      # Confusion matrix + barplots
â”‚
â”œâ”€â”€ checkpoints_*/            # Saved model checkpoints
â”œâ”€â”€ mt_validation/            # Multi-seed validation results
â”œâ”€â”€ mt_lambda_sweep/          # Î» sweep results
â””â”€â”€ figures/                  # Generated visualizations
```

---

## Quick Start

### 1. Basic Training

```bash
# Train baseline model (predicts y* only)
python train_T.py

# Check theory gradient
python analysis_T.py
```

### 2. Dynamic Kernel

```bash
# Test the dynamic kernel
python dynamic_trig_kernel.py

# Export difficulty map
python -c "from dynamic_trig_kernel import *; DynamicTrigKernel(list(range(360))).export_t_first_K()"
```

### 3. Multi-Task Validation (Main Experiment)

```bash
# Run full validation (K-real vs Shuffle, 3 seeds + Î» sweep)
python run_mt_validation.py --seeds 3 --lambda-sweep

# Generate figures
python visualize_results.py
```

### 4. Additional Analyses

```bash
# K â†” T synchronization
python sync_K_T.py

# P_vec structure (cut âŠ¥ bit)
python analysis_pvec_trig.py
```

---

## Theoretical Background

### Î©-Structure

The "world" Î© consists of:
- **X_trig**: 360 discrete angles (k/360 Ã— 2Ï€)
- **I_trig**: 8 question types (sign_sin, sign_cos, sin_ge_r, cos_ge_r for r âˆˆ {-0.5, 0, 0.5})
- **V_trig(x)**: The ideal trigonometric profile for angle x
- **question_trig(i, p)**: Evaluates question i on profile p â†’ {0, 1}

### Dynamic Kernel K

K simulates a "refinement over time" process:
- **approx_t(x)**: At time t, we have an interval approximation of sin/cos
- **val_t(x,i)**: Truth value at time t (monotone: once true, stays true)
- **t_first^K(Ïƒ)**: The first time fact Ïƒ becomes definitively true
- **halt_rank**: Classification into EARLY (t<3), MID (3â‰¤t<6), LATE (6â‰¤t<10), NEVER (doesn't stabilize)

### P_vec Structure

The latent space of T_Î¸ exhibits orthogonal structure:
- **cut**: Which quadrant (depends only on angle) â†’ 99% decodable
- **bit**: Which question type (depends only on index) â†’ 100% decodable
- **cos(W_cut, W_bit) â‰ˆ 0.06**: Nearly orthogonal

---

## What We Learned

### âœ… Validated

1. **K is not arbitrary**: Shuffling K destroys halt prediction
2. **T_Î¸ can learn K**: 99% halt accuracy when explicitly asked
3. **Sync K â†” T exists**: Pearson â‰ˆ 0.35 correlation on stabilization times
4. **P_vec is clean**: cut âŠ¥ bit in latent space

### âš ï¸ Neutral

1. **Curriculum alone doesn't help**: On this easy task, weighting by K doesn't improve Y accuracy
2. **Task is nearly saturated**: 97-99% accuracy leaves little room for improvement

### ðŸ”® Future

1. **Harder Î©**: Test on more complex structures where K matters for performance
2. **Compositional tasks**: Mini-circuits, micro-proofs, where depth matters

---

## Dependencies

```
torch>=2.0
numpy
scipy
matplotlib
scikit-learn
```

---

## Citation

Part of the **Theory of Dynamic Orders for Computation** project.

The key insight validated here:
> *"The temporal structure K, defined at the trajectory level of a dynamic kernel, 
> is real, stable, and exploitable by a neural network T 
> as soon as it becomes an explicit learning objective."*
