# Î©-Trig Experiment

> **Validation of the Î©â€“Kâ€“E dissociation framework**

---

## What Makes This Different

In standard ML, three concepts are conflated:
- Where data comes from
- What labels mean
- How we measure success

This experiment **explicitly separates** them:

| Component | Role | In Ï‰-Trig |
|-----------|------|-----------|
| **Î© (World)** | Source of instances | 360 angles Ã— 8 question types |
| **K (Oracle)** | Source of truth (static + dynamic) | y* + halt_rank |
| **E (Evaluation)** | How we judge T_Î¸ | Accuracy, sync, P_vec, ablations... |

**Key insight**: K provides signals; E judges what T_Î¸ does with them. They are not the same thing.

---

## The Setup

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      Î© (World)                              â”‚
â”‚  Generates instances Ïƒ = (angle, question)                  â”‚
â”‚  360 Ã— 8 = 2880 possible facts                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      K (Oracle)                             â”‚
â”‚  Provides truth for each Ïƒ:                                 â”‚
â”‚  - y*(Ïƒ): static truth (is sin(Î¸) â‰¥ r?)                     â”‚
â”‚  - halt_rank(Ïƒ): dynamic difficulty (EARLY/MID/LATE/NEVER)  â”‚
â”‚  For T_Î¸, K is a BLACK BOX external source of labels        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      T_Î¸ (Learner)                          â”‚
â”‚  Neural approximator trained on K's signals                 â”‚
â”‚  Input: Ïƒ â†’ Output: Å·, halt_logits                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      E (Evaluation)                         â”‚
â”‚  Multiple independent metrics:                              â”‚
â”‚  - Accuracy on y* and halt_rank                             â”‚
â”‚  - Sync: correlation t_first^K â†” t_first^T                  â”‚
â”‚  - P_vec: cut âŠ¥ bit in latent space                         â”‚
â”‚  - Theory gradient: E(T_e1) âŠ† E(T_e2)                       â”‚
â”‚  - Ablations: baseline / uniform / K-guided / shuffle       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## What This Enables

Because Î©, K, and E are separated, we can ask questions that are usually impossible:

1. **Same Î©, same K, different E**: How does T_Î¸ look under different evaluation lenses?
2. **Same Î©, different K**: What if the oracle changes (different dynamics)?
3. **K as black box**: T_Î¸ doesn't know how K computes halt_rank - it just learns to match it

---

## Key Results

### Multi-Task Validation (3 seeds)

| Condition | Y Accuracy | Halt Accuracy | Î” Halt |
|-----------|------------|---------------|--------|
| **K-real** | 99.2% Â± 0.3% | **98.7% Â± 0.4%** | - |
| **Shuffle** | 98.9% Â± 0.5% | 48.1% Â± 2.6% | **-50.6pp** |

**Shuffle test**: When we destroy K's structure (permute halt_rank assignments), T_Î¸ cannot learn halt anymore. This proves K encodes real, structured information.

### Confusion Matrix (K-real)

| True \ Pred | EARLY | MID | NEVER |
|-------------|-------|-----|-------|
| **EARLY** | 99 | 2 | 0 |
| **MID** | 2 | 135 | 0 |
| **NEVER** | 0 | 0 | 195 |

**99.1% overall** - T_Î¸ learns all classes, not just majority.

### Î» Sweep

| Î»_halt | K-real Halt | Shuffle Halt | Î” |
|--------|-------------|--------------|---|
| 0.0 | 4% | 13% | -9pp |
| 0.1 | 98% | 50% | **+48pp** |
| 0.5 | 99% | 50% | **+49pp** |
| 1.0 | 99% | 50% | **+49pp** |

---

## What We Validated

### âœ… Framework Works

1. **Î©â€“Kâ€“E separation is implementable** and produces meaningful experiments
2. **T_Î¸ can synchronize with external oracle K** (black box)
3. **K's structure matters**: shuffle destroys the halt signal (+50pp gap)
4. **Multiple evaluation lenses** (E) give consistent story

### âš ï¸ Scope Limitations

1. **This is a proof-of-concept**, not a claim about "deep dynamics"
2. **Ï‰-trig is a toy domain**: task saturates at ~99%
3. **Curriculum alone doesn't help**: weighting by K doesn't improve Y accuracy on this easy task
4. **K may be "flat"**: halt_rank could be a simple function of inputs (we don't prove otherwise)

### ðŸ”® What Would Strengthen the Claim

A future Î© where:
- Without K, T_Î¸ fails or generalizes poorly
- With K, T_Î¸ gains robustly
- And this gain is not trivial to explain

---

## Honest Summary

**What we showed**: T_Î¸ can learn to match an external oracle K, and shuffle-control proves K is structured, not noise.

**What we did NOT show**: That K is "irrÃ©ductiblement dynamique" or that K is indispensable for performance.

**The conceptual contribution**: Explicit separation of Î© (world) / K (oracle) / E (evaluation), which is rarely done in ML.

---

## File Structure

```
omega_trig/
â”œâ”€â”€ trig_kernel.py            # Î©-syntax
â”œâ”€â”€ dynamic_trig_kernel.py    # K: oracle with halt_rank
â”œâ”€â”€ dataset_trig.py           # Data from Î©
â”œâ”€â”€ model_T.py                # T_Î¸ architecture
â”‚
â”œâ”€â”€ train_T.py                # Baseline (y* only)
â”œâ”€â”€ train_T_multitask.py      # Multi-task (y* + halt_rank)
â”œâ”€â”€ train_T_curriculum.py     # Curriculum (weighted by K)
â”‚
â”œâ”€â”€ analysis_T.py             # E: theory gradient
â”œâ”€â”€ sync_K_T.py               # E: K â†” T correlation
â”œâ”€â”€ analysis_pvec_trig.py     # E: latent structure
â”‚
â”œâ”€â”€ run_mt_validation.py      # Multi-seed + Î» sweep
â”œâ”€â”€ visualize_results.py      # Confusion matrix + plots
â””â”€â”€ figures/                  # Generated visualizations
```

---

## Quick Start

```bash
# 1. Run the main experiment
python run_mt_validation.py --seeds 3 --lambda-sweep

# 2. Generate figures
python visualize_results.py

# 3. Check sync K â†” T
python sync_K_T.py
```

---

## Dependencies

```
torch>=2.0
numpy
scipy
matplotlib
scikit-learn
```

---

## Citation

Part of the **Theory of Dynamic Orders for Computation** project.

> *The key contribution is the explicit separation of Î© (world), K (oracle), and E (evaluation) â€”
> a modular architecture that enables experiments about T_Î¸'s relationship to truth,
> not just its loss on labels.*
